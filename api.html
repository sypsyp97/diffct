

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>API Reference &mdash; diffct  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Examples" href="examples.html" />
    <link rel="prev" title="Getting Started" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            diffct
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallel-beam-operators">Parallel Beam Operators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction"><code class="docutils literal notranslate"><span class="pre">ParallelProjectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">ParallelProjectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">ParallelProjectorFunction.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction"><code class="docutils literal notranslate"><span class="pre">ParallelBackprojectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">ParallelBackprojectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">ParallelBackprojectorFunction.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#fan-beam-operators">Fan Beam Operators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.FanProjectorFunction"><code class="docutils literal notranslate"><span class="pre">FanProjectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">FanProjectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">FanProjectorFunction.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction"><code class="docutils literal notranslate"><span class="pre">FanBackprojectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">FanBackprojectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">FanBackprojectorFunction.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cone-beam-operators">Cone Beam Operators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction"><code class="docutils literal notranslate"><span class="pre">ConeProjectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">ConeProjectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">ConeProjectorFunction.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction"><code class="docutils literal notranslate"><span class="pre">ConeBackprojectorFunction</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.backward"><code class="docutils literal notranslate"><span class="pre">ConeBackprojectorFunction.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.forward"><code class="docutils literal notranslate"><span class="pre">ConeBackprojectorFunction.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#usage-notes">Usage Notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">diffct</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">API Reference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="api-reference">
<h1>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading"></a></h1>
<p>This section provides comprehensive documentation for all differentiable CT operators in <cite>diffct</cite>. Each function is implemented as a PyTorch autograd Function, enabling seamless gradient computation through the CT reconstruction pipeline.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>The <cite>diffct</cite> library provides six main differentiable operators organized by geometry type:</p>
<ul class="simple">
<li><p><strong>Parallel Beam (2D):</strong> Traditional parallel-beam CT geometry</p></li>
<li><p><strong>Fan Beam (2D):</strong> Fan-beam geometry with configurable source-detector setup</p></li>
<li><p><strong>Cone Beam (3D):</strong> Full 3D cone-beam geometry for volumetric reconstruction</p></li>
</ul>
<p>Each geometry type includes both forward projection and backprojection operators that are fully differentiable and CUDA-accelerated.</p>
</section>
<section id="parallel-beam-operators">
<h2>Parallel Beam Operators<a class="headerlink" href="#parallel-beam-operators" title="Link to this heading"></a></h2>
<p>The parallel beam geometry assumes parallel X-ray beams, commonly used in synchrotron CT and some medical CT scanners.</p>
<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelProjectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">ParallelProjectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelProjectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelProjectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 2D parallel beam forward projection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph
ray-tracing algorithm for parallel beam CT geometry. The forward pass computes
the sinogram from a 2D image using parallel beam geometry. The backward pass
computes gradients using the adjoint backprojection operation. Requires
CUDA-capable hardware and a properly configured CUDA environment; all input
tensors must reside on the same CUDA device.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParallelProjectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create a 2D image with gradient tracking</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define projection parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_detectors</span> <span class="o">=</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_spacing</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute forward projection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">projector</span> <span class="o">=</span> <span class="n">ParallelProjectorFunction</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">projector</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="n">num_detectors</span><span class="p">,</span> <span class="n">detector_spacing</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute loss and gradients</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">sinogram</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (128, 128)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelProjectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_sinogram</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelProjectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelProjectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.forward" title="diffct.differentiable.ParallelProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.forward" title="diffct.differentiable.ParallelProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.backward" title="diffct.differentiable.ParallelProjectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.ParallelProjectorFunction.forward" title="diffct.differentiable.ParallelProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelProjectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_detectors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_spacing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelProjectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelProjectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 2D parallel beam forward projection (Radon transform) of
an image using CUDA acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> (<em>torch.Tensor</em>) – 2D input image tensor of shape (H, W), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_angles,), must be on the same CUDA device as <cite>image</cite>.</p></li>
<li><p><strong>num_detectors</strong> (<em>int</em>) – Number of detector elements in the sinogram (columns).</p></li>
<li><p><strong>detector_spacing</strong> (<em>float</em><em>, </em><em>optional</em>) – Physical spacing between detector elements (default: 1.0).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>sinogram</strong> – 2D tensor of shape (num_angles, num_detectors) containing the forward projection (sinogram) on the same device as <cite>image</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate ray tracing and bilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">ParallelProjectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">image</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mf">1.0</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelBackprojectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">ParallelBackprojectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelBackprojectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelBackprojectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 2D parallel beam backprojection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph ray-tracing
algorithm for parallel beam backprojection. The forward pass computes a 2D
reconstruction from sinogram data using parallel beam backprojection, and the
backward pass computes gradients via forward projection as the adjoint operation.
Requires CUDA-capable hardware and consistent device placements.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParallelBackprojectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">180</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recon</span> <span class="o">=</span> <span class="n">ParallelBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinogram</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">recon</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sinogram</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (180, 128)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelBackprojectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelBackprojectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelBackprojectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.forward" title="diffct.differentiable.ParallelBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.forward" title="diffct.differentiable.ParallelBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.backward" title="diffct.differentiable.ParallelBackprojectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.ParallelBackprojectorFunction.forward" title="diffct.differentiable.ParallelBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ParallelBackprojectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sinogram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_spacing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">H</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">W</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ParallelBackprojectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ParallelBackprojectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 2D parallel beam backprojection (adjoint Radon
transform) of a sinogram using CUDA acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sinogram</strong> (<em>torch.Tensor</em>) – 2D input sinogram tensor of shape (num_angles, num_detectors), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_angles,), must be on the same CUDA device as <cite>sinogram</cite>.</p></li>
<li><p><strong>detector_spacing</strong> (<em>float</em><em>, </em><em>optional</em>) – Physical spacing between detector elements (default: 1.0).</p></li>
<li><p><strong>H</strong> (<em>int</em><em>, </em><em>optional</em>) – Height of the output reconstruction image (default: 128).</p></li>
<li><p><strong>W</strong> (<em>int</em><em>, </em><em>optional</em>) – Width of the output reconstruction image (default: 128).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>reco</strong> – 2D tensor of shape (H, W) containing the reconstructed image on the same device as <cite>sinogram</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate ray tracing and bilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">180</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reco</span> <span class="o">=</span> <span class="n">ParallelBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">sinogram</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="fan-beam-operators">
<h2>Fan Beam Operators<a class="headerlink" href="#fan-beam-operators" title="Link to this heading"></a></h2>
<p>Fan beam geometry uses a point X-ray source with a fan-shaped beam, typical in medical CT scanners.</p>
<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.FanProjectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">FanProjectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanProjectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanProjectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="id2">
<h3>Summary<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 2D fan beam forward projection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph
ray-tracing algorithm for fan beam geometry, where rays diverge from a point
X-ray source to a linear detector array. The forward pass computes sinograms
using divergent beam geometry, and the backward pass computes gradients via
adjoint backprojection.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">FanProjectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">FanProjectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">sinogram</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (256, 256)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.FanProjectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_sinogram</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanProjectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanProjectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.forward" title="diffct.differentiable.FanProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.forward" title="diffct.differentiable.FanProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.backward" title="diffct.differentiable.FanProjectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.FanProjectorFunction.forward" title="diffct.differentiable.FanProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.FanProjectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_detectors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_spacing</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sdd</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sid</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanProjectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanProjectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 2D fan beam forward projection of an image using CUDA
acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> (<em>torch.Tensor</em>) – 2D input image tensor of shape (H, W), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_angles,), must be on the same CUDA device as <cite>image</cite>.</p></li>
<li><p><strong>num_detectors</strong> (<em>int</em>) – Number of detector elements in the sinogram (columns).</p></li>
<li><p><strong>detector_spacing</strong> (<em>float</em>) – Physical spacing between detector elements.</p></li>
<li><p><strong>sdd</strong> (<em>float</em>) – Source-to-Detector Distance (SDD). The total distance from the X-ray
source to the detector, passing through the isocenter.</p></li>
<li><p><strong>sid</strong> (<em>float</em>) – Source-to-Isocenter Distance (SID). The distance from the X-ray
source to the center of rotation (isocenter).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>sinogram</strong> – 2D tensor of shape (num_angles, num_detectors) containing the fan beam sinogram on the same device as <cite>image</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Fan beam geometry uses divergent rays from a point source to the detector.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate ray tracing and bilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">FanProjectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">image</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.FanBackprojectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">FanBackprojectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanBackprojectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanBackprojectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="id3">
<h3>Summary<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 2D fan beam backprojection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph
ray-tracing algorithm for fan beam backprojection. Implements the adjoint
of the fan beam projection operator, distributing sinogram values back into
the reconstruction volume along divergent ray paths. The forward pass
computes reconstruction from sinogram data, and the backward pass computes
gradients via forward projection.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">FanBackprojectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">360</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recon</span> <span class="o">=</span> <span class="n">FanBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinogram</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">recon</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sinogram</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (360, 512)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.FanBackprojectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanBackprojectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanBackprojectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.forward" title="diffct.differentiable.FanBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.forward" title="diffct.differentiable.FanBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.backward" title="diffct.differentiable.FanBackprojectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.FanBackprojectorFunction.forward" title="diffct.differentiable.FanBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.FanBackprojectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sinogram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_spacing</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">H</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">W</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sdd</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sid</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#FanBackprojectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.FanBackprojectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 2D fan beam backprojection of a sinogram using CUDA
acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sinogram</strong> (<em>torch.Tensor</em>) – 2D input fan beam sinogram tensor of shape (num_angles, num_detectors), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_angles,), must be on the same CUDA device as <cite>sinogram</cite>.</p></li>
<li><p><strong>detector_spacing</strong> (<em>float</em>) – Physical spacing between detector elements.</p></li>
<li><p><strong>H</strong> (<em>int</em>) – Height of the output reconstruction image.</p></li>
<li><p><strong>W</strong> (<em>int</em>) – Width of the output reconstruction image.</p></li>
<li><p><strong>sdd</strong> (<em>float</em>) – Source-to-Detector Distance (SDD). The total distance from the X-ray
source to the detector, passing through the isocenter.</p></li>
<li><p><strong>sid</strong> (<em>float</em>) – Source-to-Isocenter Distance (SID). The distance from the X-ray
source to the center of rotation (isocenter).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>reco</strong> – 2D tensor of shape (H, W) containing the reconstructed image on the same device as <cite>sinogram</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Fan beam geometry uses divergent rays from a point source to the detector.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate ray tracing and bilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sinogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">360</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reco</span> <span class="o">=</span> <span class="n">FanBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">sinogram</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">500.0</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="cone-beam-operators">
<h2>Cone Beam Operators<a class="headerlink" href="#cone-beam-operators" title="Link to this heading"></a></h2>
<p>Cone beam geometry extends fan beam to 3D with a cone-shaped X-ray beam for volumetric reconstruction.</p>
<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.ConeProjectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">ConeProjectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeProjectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeProjectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="id4">
<h3>Summary<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 3D cone beam forward projection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph
ray-tracing algorithm for 3D cone beam geometry. Rays emanate from a point
X-ray source to a 2D detector array capturing volumetric projection data.
The forward pass computes 3D projections, and the backward pass computes
gradients via adjoint 3D backprojection. Requires significant GPU memory.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConeProjectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">volume</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">projections</span> <span class="o">=</span> <span class="n">ConeProjectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">volume</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">projections</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">volume</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (128, 128, 128)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ConeProjectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_sinogram</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeProjectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeProjectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.forward" title="diffct.differentiable.ConeProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.forward" title="diffct.differentiable.ConeProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.backward" title="diffct.differentiable.ConeProjectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.ConeProjectorFunction.forward" title="diffct.differentiable.ConeProjectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ConeProjectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">volume</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">det_u</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">det_v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">du</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sdd</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sid</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeProjectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeProjectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 3D cone beam forward projection of a volume using CUDA
acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>volume</strong> (<em>torch.Tensor</em>) – 3D input volume tensor of shape (D, H, W), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_views,), must be on the same CUDA device as <cite>volume</cite>.</p></li>
<li><p><strong>det_u</strong> (<em>int</em>) – Number of detector elements along the u-axis (width).</p></li>
<li><p><strong>det_v</strong> (<em>int</em>) – Number of detector elements along the v-axis (height).</p></li>
<li><p><strong>du</strong> (<em>float</em>) – Physical spacing between detector elements along the u-axis.</p></li>
<li><p><strong>dv</strong> (<em>float</em>) – Physical spacing between detector elements along the v-axis.</p></li>
<li><p><strong>sdd</strong> (<em>float</em>) – Source-to-Detector Distance (SDD). The total distance from the X-ray
source to the detector, passing through the isocenter.</p></li>
<li><p><strong>sid</strong> (<em>float</em>) – Source-to-Isocenter Distance (SID). The distance from the X-ray
source to the center of rotation (isocenter).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>sino</strong> – 3D tensor of shape (num_views, det_u, det_v) containing the cone beam projections on the same device as <cite>volume</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Cone beam geometry uses a point source and a 2D detector array.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate 3D ray tracing and trilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">volume</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sino</span> <span class="o">=</span> <span class="n">ConeProjectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">volume</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="diffct.differentiable.ConeBackprojectorFunction">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">diffct.differentiable.</span></span><span class="sig-name descname"><span class="pre">ConeBackprojectorFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeBackprojectorFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeBackprojectorFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<section id="id5">
<h3>Summary<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>PyTorch autograd function for differentiable 3D cone beam backprojection.</p>
<p class="rubric">Notes</p>
<p>Provides a differentiable interface to the CUDA-accelerated Siddon-Joseph
ray-tracing algorithm for 3D cone beam backprojection. The forward pass
computes a 3D reconstruction from cone beam projection data using
backprojection as the adjoint operation. The backward pass computes gradients
via 3D cone beam forward projection. Requires CUDA-capable hardware and
consistent device placements.</p>
<p>This operation may be memory- and computationally-intensive due to 3D geometry.
Consider using gradient checkpointing, smaller volumes, or distributed computing
for large-scale applications, and ensure sufficient GPU memory is available.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">diffct.differentiable</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConeBackprojectorFunction</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">projections</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">360</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">du</span><span class="p">,</span> <span class="n">dv</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sdd</span><span class="p">,</span> <span class="n">sid</span> <span class="o">=</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">backprojector</span> <span class="o">=</span> <span class="n">ConeBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">volume</span> <span class="o">=</span> <span class="n">backprojector</span><span class="p">(</span><span class="n">projections</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">du</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="n">sdd</span><span class="p">,</span> <span class="n">sid</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">volume</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Projection gradient shape: </span><span class="si">{</span><span class="n">projections</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (360, 256, 256)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ConeBackprojectorFunction.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeBackprojectorFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeBackprojectorFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.forward" title="diffct.differentiable.ConeBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.forward" title="diffct.differentiable.ConeBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.backward" title="diffct.differentiable.ConeBackprojectorFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#diffct.differentiable.ConeBackprojectorFunction.forward" title="diffct.differentiable.ConeBackprojectorFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="diffct.differentiable.ConeBackprojectorFunction.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sinogram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">D</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">H</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">W</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">du</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sdd</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sid</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/diffct/differentiable.html#ConeBackprojectorFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#diffct.differentiable.ConeBackprojectorFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Compute the 3D cone beam backprojection of a projection sinogram
using CUDA acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sinogram</strong> (<em>torch.Tensor</em>) – 3D input cone beam projection tensor of shape (num_views, det_u, det_v), must be on a CUDA device and of type float32.</p></li>
<li><p><strong>angles</strong> (<em>torch.Tensor</em>) – 1D tensor of projection angles in radians, shape (num_views,), must be on the same CUDA device as <cite>sinogram</cite>.</p></li>
<li><p><strong>D</strong> (<em>int</em>) – Depth (z-dimension) of the output reconstruction volume.</p></li>
<li><p><strong>H</strong> (<em>int</em>) – Height (y-dimension) of the output reconstruction volume.</p></li>
<li><p><strong>W</strong> (<em>int</em>) – Width (x-dimension) of the output reconstruction volume.</p></li>
<li><p><strong>du</strong> (<em>float</em>) – Physical spacing between detector elements along the u-axis.</p></li>
<li><p><strong>dv</strong> (<em>float</em>) – Physical spacing between detector elements along the v-axis.</p></li>
<li><p><strong>sdd</strong> (<em>float</em>) – Source-to-Detector Distance (SDD). The total distance from the X-ray
source to the detector, passing through the isocenter.</p></li>
<li><p><strong>sid</strong> (<em>float</em>) – Source-to-Isocenter Distance (SID). The distance from the X-ray
source to the center of rotation (isocenter).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>vol</strong> – 3D tensor of shape (D, H, W) containing the reconstructed volume on the same device as <cite>sinogram</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>All input tensors must be on the same CUDA device.</p></li>
<li><p>The operation is fully differentiable and supports autograd.</p></li>
<li><p>Cone beam geometry uses a point source and a 2D detector array.</p></li>
<li><p>Uses the Siddon-Joseph algorithm for accurate 3D ray tracing and trilinear interpolation.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">projections</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">360</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vol</span> <span class="o">=</span> <span class="n">ConeBackprojectorFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">projections</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1500.0</span><span class="p">,</span> <span class="mf">1000.0</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="usage-notes">
<h2>Usage Notes<a class="headerlink" href="#usage-notes" title="Link to this heading"></a></h2>
<p><strong>Memory Management:</strong>
- All operators work with GPU tensors for optimal performance
- Ensure sufficient GPU memory for your problem size
- Use <code class="docutils literal notranslate"><span class="pre">torch.cuda.empty_cache()</span></code> if encountering memory issues</p>
<p><strong>Gradient Computation:</strong>
- All operators support automatic differentiation
- Gradients flow through both forward and backward passes
- Set <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> on input tensors to enable gradients</p>
<p><strong>Performance Considerations:</strong>
- Use contiguous tensors for optimal memory access
- Consider batch processing for multiple reconstructions
- Profile your code to identify bottlenecks</p>
<p><strong>Coordinate Systems:</strong>
- Image/volume coordinates: (0,0) at top-left corner
- Detector coordinates: centered at detector array center
- Rotation: counter-clockwise around z-axis (right-hand rule)</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="examples.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Yipeng Sun.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>